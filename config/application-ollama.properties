# The local URL where Ollama is running
#spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.base-url=http://ollama:11434

# Specify which model you pulled (e.g., llama3, mistral, deepseek-r1)
# for local system
#spring.ai.ollama.chat.options.model=llama3
# for docker
spring.ai.ollama.chat.options.model=phi3
# Optional: Configure creativity (Temperature 0.0 is more technical/stable)
spring.ai.ollama.chat.options.temperature=0.7



############# description of start and stop ollama via commands ###############
# ollama  ---> To show important commands
# ollama pull llama3  --->   Pull a Model: Open your terminal and download the model you want to use
# ollama serve  ---->  Run the Server: Ensure Ollama is running in the background. You can start it with
# Available Commands:
#  serve   --->    Start Ollama
#  create   --->    Create a model
#  show     --->    Show information for a model
#  run      --->    Run a model
#  stop      --->   Stop a running model
#  pull     --->    Pull a model from a registry
#  push     --->    Push a model to a registry
#  signin    --->   Sign in to ollama.com
#  signout   --->   Sign out from ollama.com
#  list     --->    List models
#  ps       --->    List running models
#  cp      --->     Copy a model
#  rm       --->    Remove a model
#  launch   --->    Launch an integration with Ollama
#  help    --->     Help about any command
#
#Flags:
#  -h, --help    --->   help for ollama
#  -v, --version  --->  Show version information

################### pull ollama phi3
docker exec -it interview-ollama ollama pull phi3